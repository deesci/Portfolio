{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P01 DPanchal\n",
    "\n",
    "---\n",
    "- Python List of List, without help of NumPy and Pandas \n",
    " \n",
    "---\n",
    "\n",
    "\n",
    "# Prepare and Analyse with Python, data to find type of post that receives more comments\n",
    "**Hacker news** is popular site where user-submitted stories are voted and commented upon. We have a set of posts in cvs file containing posts. We are going to analyse this data using Python by making a list of list from csv. The aim is to come up with the *type* of post that receives more comments, for example `'show HN'` or `'ask HN'`. \n",
    "\n",
    "### Source data\n",
    "The data we are using is a randomised reduced collection of about twenty thousand rows. It has *seven columns*, 'id', 'title', 'url', 'num_points', 'num_comments', 'author', and 'created_at'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n",
      "[['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52'], ['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30'], ['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20'], ['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']]\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "opened_file = open('hacker_news.csv')\n",
    "read_file = reader(opened_file)\n",
    "hn = list(read_file)\n",
    "headers = hn[0]\n",
    "# treat header\n",
    "hn = hn[1:]\n",
    "\n",
    "print(headers)\n",
    "print(hn[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Filter Posts into: Ask HN and Show HN\n",
    "\n",
    "Let us explore data for relevant posts by filtering posts starting with Ask HN and Show HN.\n",
    "\n",
    "There are `1744 ask`, and `1162 show` posts. Remaining posts are 17194.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744\n",
      "1162\n",
      "17194\n"
     ]
    }
   ],
   "source": [
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "\n",
    "for row in hn:\n",
    "    title = row[1]\n",
    "    title_lower = title.lower()\n",
    "    # use startswith to segregate \n",
    "    if title_lower.startswith('ask hn'):\n",
    "        ask_posts.append(row)\n",
    "    elif title_lower.startswith('show hn'):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "\n",
    "print(len(ask_posts))\n",
    "print(len(show_posts))\n",
    "print(len(other_posts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corresponding number of comments\n",
    "Now that we have isolated Ask and Show posts into their own list, lets look at their number of comments.\n",
    "\n",
    "**14 compared to 10** - The below analysis shows that there are **14** number of comments on *Ask post*, compared to **10.3** on *Show post*, on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.038417431192661\n",
      "10.31669535283993\n"
     ]
    }
   ],
   "source": [
    "total_ask_comments = 0\n",
    "\n",
    "# iterate and get total number of ask comments\n",
    "for row in ask_posts:\n",
    "    comments = int(row[4])\n",
    "    total_ask_comments += comments\n",
    "\n",
    "avg_ask_comments = total_ask_comments/len(ask_posts)\n",
    "print(avg_ask_comments)\n",
    " \n",
    "# do the same for show comments\n",
    "\n",
    "total_show_comments = 0\n",
    "\n",
    "# iterate and get total number of Show comments\n",
    "for row in show_posts:\n",
    "    comments = int(row[4])\n",
    "    total_show_comments += comments\n",
    "\n",
    "avg_show_comments = total_show_comments/len(show_posts)\n",
    "print(avg_show_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare comments by the hour\n",
    "We are going to analyse the ask posts, this time in relation to the hour. \n",
    "\n",
    "In below, we are counting comments for each hour, as well as total comments for each hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'06': 44, '18': 109, '03': 54, '00': 55, '04': 47, '19': 110, '22': 71, '02': 58, '12': 73, '08': 48, '01': 60, '05': 46, '09': 45, '11': 58, '14': 107, '21': 109, '13': 85, '23': 68, '20': 80, '15': 116, '07': 34, '17': 100, '10': 59, '16': 108}\n",
      "{'06': 397, '18': 1439, '03': 421, '00': 447, '04': 337, '19': 1188, '22': 479, '02': 1381, '12': 687, '08': 492, '01': 683, '05': 464, '09': 251, '11': 641, '14': 1416, '21': 1745, '13': 1253, '23': 543, '20': 1722, '15': 4477, '07': 267, '17': 1146, '10': 793, '16': 1814}\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "result_list= []\n",
    "\n",
    "\n",
    "for row in ask_posts:\n",
    "    result_list.append([row[6], int(row[4])])\n",
    "\n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "\n",
    "for row in result_list:\n",
    "    hour = row[0]\n",
    "    # use datetime to extract hour\n",
    "    hour_object = dt.datetime.strptime(hour , '%m/%d/%Y %H:%M')\n",
    "    hour_only = dt.datetime.strftime(hour_object, '%H')\n",
    "    if hour_only not in counts_by_hour:\n",
    "        counts_by_hour[hour_only] = 1\n",
    "        comments_by_hour[hour_only] = int(row[1])\n",
    "    else:\n",
    "        counts_by_hour[hour_only] += 1\n",
    "        comments_by_hour[hour_only] += int(row[1])\n",
    "    \n",
    "print(counts_by_hour)    \n",
    "print(comments_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Average comments for every hour\n",
    "Let us summarise count and comments together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['06', 9.022727272727273], ['18', 13.20183486238532], ['03', 7.796296296296297], ['00', 8.127272727272727], ['04', 7.170212765957447], ['19', 10.8], ['22', 6.746478873239437], ['02', 23.810344827586206], ['12', 9.41095890410959], ['08', 10.25], ['01', 11.383333333333333], ['05', 10.08695652173913], ['09', 5.5777777777777775], ['11', 11.051724137931034], ['14', 13.233644859813085], ['21', 16.009174311926607], ['13', 14.741176470588234], ['23', 7.985294117647059], ['20', 21.525], ['15', 38.5948275862069], ['07', 7.852941176470588], ['17', 11.46], ['10', 13.440677966101696], ['16', 16.796296296296298]]\n"
     ]
    }
   ],
   "source": [
    "avg_by_hour = []\n",
    "\n",
    "for hour in counts_by_hour:\n",
    "    avg_by_hour.append([hour, int(comments_by_hour[hour])/int(counts_by_hour[hour]) ])\n",
    "    \n",
    "    \n",
    "print(avg_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange from high to low\n",
    "Now we got the figures, let us arrange in order so we can see the top ones in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.022727272727273, '06'], [13.20183486238532, '18'], [7.796296296296297, '03'], [8.127272727272727, '00'], [7.170212765957447, '04'], [10.8, '19'], [6.746478873239437, '22'], [23.810344827586206, '02'], [9.41095890410959, '12'], [10.25, '08'], [11.383333333333333, '01'], [10.08695652173913, '05'], [5.5777777777777775, '09'], [11.051724137931034, '11'], [13.233644859813085, '14'], [16.009174311926607, '21'], [14.741176470588234, '13'], [7.985294117647059, '23'], [21.525, '20'], [38.5948275862069, '15'], [7.852941176470588, '07'], [11.46, '17'], [13.440677966101696, '10'], [16.796296296296298, '16']]\n",
      "\n",
      "\n",
      "[[38.5948275862069, '15'], [23.810344827586206, '02'], [21.525, '20'], [16.796296296296298, '16'], [16.009174311926607, '21'], [14.741176470588234, '13'], [13.440677966101696, '10'], [13.233644859813085, '14'], [13.20183486238532, '18'], [11.46, '17'], [11.383333333333333, '01'], [11.051724137931034, '11'], [10.8, '19'], [10.25, '08'], [10.08695652173913, '05'], [9.41095890410959, '12'], [9.022727272727273, '06'], [8.127272727272727, '00'], [7.985294117647059, '23'], [7.852941176470588, '07'], [7.796296296296297, '03'], [7.170212765957447, '04'], [6.746478873239437, '22'], [5.5777777777777775, '09']]\n"
     ]
    }
   ],
   "source": [
    "swap_avg_by_hour = []\n",
    "\n",
    "for hour in avg_by_hour:\n",
    "    swap_avg_by_hour.append([hour[1], hour[0]])\n",
    "print(swap_avg_by_hour)\n",
    "\n",
    "sorted_swap = sorted(swap_avg_by_hour, reverse=True)\n",
    "print('\\n')\n",
    "print(sorted_swap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Top 5\n",
    "### Here are the top hours for Ask Post comments\n",
    "We started with full dataset, filtered out Ask and Post, counted comments, looked at hours, and **finally** we have result of analysis. \n",
    "\n",
    "The result we found is that the *Posts* belonging to the **15:00** hour gets the highest number of comments on average, around *38*. Followed by **02:00**, around *23* comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hours for Ask Posts Comments\n",
      "15:00: 38.59 average comments per post\n",
      "02:00: 23.81 average comments per post\n",
      "20:00: 21.52 average comments per post\n",
      "16:00: 16.80 average comments per post\n",
      "21:00: 16.01 average comments per post\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Top 5 Hours for Ask Posts Comments')\n",
    "\n",
    "i=1\n",
    "for hour in sorted_swap:\n",
    "    if i < 6:\n",
    "        a= '{}:00: {:.2f} average comments per post'.format(hour[1],hour[0])\n",
    "        i += 1\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In general, the Afternoon, and late evening dominate the comments compared to other hours of the day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
